\section{Services for Data Quality Analysis (SDQA)}
\label{sec:sdqa}

\subsection{Key Requirements}

SQDA is a set of loosely coupled services intended to service LSST's quality assessement needs through all phases of Construction, Commissioning and Operations. Consumers of these services may include developers, facility staff, DAC (eg. L3) users, and the general LSST science user community. Use of these services is intended for routine characterisation, fault detection and fault diagnosis. 

\begin{itemize}
\item SDQA shall provide sevices for science data quality analysis of Level 1, 2, and Calibration Processing pipelines.

\item SDQA shall provide services to support software development in Construction, Commissioning and Operations.

\item SDQA shall provide for the visualization, analysis and monitoring capabilities needed for common science quality data analysis usecases. Its inputs may be gathered from SDQA services, the production pipelines, engineering data sources and non-LSST data sources. 

\item SDQA shall have the flexibility to support execution of ad-hoc (user-driven) tests and analyses of ad-hoc datasets (provided they are supported by the LSST stack) within a standard framework.  

\item SDQA shall support usecases involving interactive ``drill-down'' of QA data exposed through its visualisation interfaces. 

\item SDQA shall allow for notifications to be issued when monitoring quantities that exceed their permissible thresholds and/or have degraded over historical values. 

\item SDQA shall be able to collect and harvest the outputs and logs of execution of the production pipelines, and extract and expose metrics from these logs. 

\item SQDA shall make provision to store outputs that are not stored through other LSST data access services. 

\item SDQA should be deployable as high-reliability scalable services for production as well as allow for core data assessment functionality to be executed on a developer's local machine.

\item SDQA shall be architected in a manner that would enable it to be deployable on standard cloud architectures outside of the LSST facilities. 


\end{itemize}


\subsection{Key Tasks for Each Level of QA}

SDQA system will provide a framework that is capable of monitoring QA
information at four different stages of capability and maturity:

\begin{itemize}
\item QA Level 0 - Testing and Validation of the DM sub-system in pre-commissioning 
\item QA Level 1 - Real-time data quality and system assesment during commissioning + operations (also, forensics)
\item QA Level 2 - Quality assessment of Data Releases (also, forensics) 
\item QA Level 3 - Ability for the community to evaluate the data quality of their own analyses. These should made available as well-documented and deployable versions of core QA Level 0--2 services. 
\end{itemize}

\begin{note}[Figure summarising QA key tasks]
Summary figure under construction
\end{note}

\subsubsection{Phase? 0 QA - QA-0}

The first step to good quality data is good quality software. The
purpose of QA0 services is to enable testing of the DM software during
pre-commissioning as well as validate software improvements during
commissioning and operations, quantifying the software performance
against known and/or expected outputs.

The core capabilities of QA 0 services are:

\paragraph{Continuous Integration Services}
\label{sec:qaCI}
\begin{itemize}

\item Continuous integration services compile code to uncover syntax errors

\item Builds of references (tags, branches) can happen on a schedule, on developer request or on development events (eg merge to master)

\item SDQA provides CI services on multiple reference platforms and uses OS portability testing as a way to ensure the codebase is well engineered for future use. 

\end{itemize}

\paragraph{Test execution harness}
\label{sec:qaTestharness}
\begin{itemize}

\item A test execution harness runs tests (such as functional unit
  tests) on a regular cadence (eg nightly/weekly/monthly) to allow
  basic functional checkout of the code

\item Results from such tests are exposed in such a way to allow
  summary reports and meaningful failure notifications

\end{itemize}

\paragraph{Validation Metrics Code}
\label{sec:qaValidate}
\begin{itemize}

\item During Construction, progress towards meeting DM subsystem requirements revolve around the Key Performance Metrics (KPMs) outlined in LDM-240. SDQA implements code to calculate these KPMs. Consult \emph{ reference to KPM Verification document} for a list of those metrics and how (and by whom and on what) they will be calculated. 

\item Additional metrics must be calculated to be met in order for the DM subsystem to demonstrate its operational readiness. The list of those metrics and how (and by whom) they will be calculated will be in \emph{reference to DM Verification Plan CoDR document}. In terms of QA infrastructure, these metrics will not require different capabilities than the KPMs.

\item Validation code will be implemented in such a way that it can run inline with normal pipeline processing on developer's laptops. 

\item Additional metrics may be devised during construction that are helpful to development or algorithm characterisation. SDQA will provide ways of executing that code in a similar way to KPMs, but apps developers may need to contribute the code (or at least document the algorithmic approach) to calculate those metrics.

\end{itemize}

\paragraph{Computational Metrics}
\label{sec:qaComputational}
\begin{itemize}

\item While the scope of this document is the scientific aspects of the pipelines, SDQA must also service non-scientific KPMs such as computational performance characterisation. 

\item SDQA will provide a capability to instrument the production  pipelines to calculate computational performance metrics

\item The computational performance metrics that SQDA calculates will be in practice surrogates for the actual computational performance in production since those will depend on the DAC architecture. The purpose of calculating those as part of SDQA is to continuously monitor relative performance to alert the developers that a regression has occured. 

\item SDQA can calculate modeled system performance from the surrogate computational metrics if a model is provided to it (eg from Architecture). 

\item A library of those instrumentations will be provided so that they can be mixed and matched to pipelines depending on the performance metric of interest. 

\end{itemize}


\paragraph{Curated Datasets}
\label{sec:qaCurateddata}
\begin{itemize}

\item Part of the process for validating the software and its performance is selecting rich but targeted standardized data sets to generate directly comparable metrics between different versions of the software. 

\item SDQA will select and curate a combination of simulated and precursor datasets that are modest enough for ``canary'' test runs but rich enough to characterise the envelope of algorithmic performance. 

\item SDQA will ``publish'' (make available) these datasets so developers can run the validation tests directly against them in their own environments. 

\end{itemize}


\paragraph{SQUASH - Science Quality Analysis Harness}
\label{sec:qaSquash}

SQUASH is a QA-as-a-service architecture that comprises of the
following elements:

\begin{enumerate}
\item The execution of simple pipeline workflows for the purposes of QA

\item The construction of those QA workflows with an emphasis on usability (as opposed to performance)

\item The collection and exposure of the results of those runs for further retrieval and analysis

\item A monitoring system to detect threshold trespass and excursions from past trends

\item Exposure of the data for retrieval and to interactive analysis tooling

\end{enumerate}

\begin{itemize}

\item As construction progresses, first-party DM systems to underwrite the functions of SQUASH will become production ready. In the meantime, basic implementations of minimum viable functionality may be done with boostrap or off-the-shelf solutions either as an interim measure or, in some cases, a more lightweight solution. 

\item A simple example of a ``factory'' analysis based on SQUASH is ``Calculate the astrometric repeatability on this dataset; display the trend; drill down to to show the historgram of the points that went into calculating this trend''. 

\item An advanced example of a bespoke analysis based on SQUASH is “Display a three-color diagram of the sources in this run; compute the width of point sources in the selected - eg. blue - part of the locus''

\item SQUASH will likely expose results to the LSST Science User Interface for advanced interaction scenarios (both because of the SUI team's front-end expertise but also because they are likely to be similar to science-driven interactions in intent and in execution)

\end{itemize}



\begin{note}[QA-0 ends here]]
Rest of QA-0 are notes, questions and bits that still need tidying
\end{note}

\paragraph{Do these belong in this document? - FE}

Note that the scope of the document according to the abstract is (emphasis FE):

\begin{quote}

  The LSST Data Management subsystem’s responsibilities include the
  design, implemen- tation, deployment and execution of software
  pipelines \textbf{necessary to generate these data products}. This
  document, in conjunction with the UML Use Case model (LDM-134),
  describes the design of the \textbf{scientific} aspects of those
  pipelines.
  
\end{quote}

\textbf{FE:}
A lot of the QA section seems to verge on being beyond a discussion of the scientific aspects of of the pipelines necessary to generate DPDD products. I worry this is the origin of my confusion on the scope and right level of detail for this document, and the potentially inconsistent tone of the QA section compared to the pipelines one. 

\begin{itemize}
\item Repository management? Developer communication services? These don't seem to be apps specific?
\end{itemize}

\subsubsection{QA1}
Data quality in real time during commissioning and operations. Analyzes the data stream in near-real time, information about observing conditions (sky brightness, transparency, seeing, magnitude limit) as well as characterize subtle deterioration in system performance.

Validating the operational system.

Main components reused from Level QA0:
\begin{enumerate}
\item Library of validation metrics codes
\item Instrumentation capability for computational performance metrics
\item Library of “instrumentations”
\item Interface to results (including visualization)
\item Curated datasets to use in tests
\item Capability for interactive analysis of failures (drilldown into existing tests, ad hoc tests, ad hoc afterburners) – some has to come from Science Pipelines or SUIT but SQuaRE provides examples
\item Connection from analysis toolkit to validation metrics
\item QA data access service (including ingesting it as well as querying it)
\end{enumerate}

Main components reused from Level QA0: All.

Additional components required for Level QA1 services:

\begin{enumerate}
\item Harness for analyzing alert contents (and perhaps format)
\item Faster metrics codes to meet overall 60 second performance requirement for alert publication (but not necessarily for all QA processing, which must meet only throughput requirements)
\item Additional metrics/instrumentation codes (that must not disturb production system, including its performance, when dynamically inserted)
\item Output interface to “comfort” display (aggregation, trending, etc.)
\item Output interface to automated systems (drop alerts, reschedule field, etc.)
\item Correlator between telemetry streams and metrics
\item Input interface from sources of data not already present in Prompt Processing system
\item Fake source injection and analysis
\item Metrics codes specific for calibration/engineering/special- purpose images
\end{enumerate}

\subsubsection{QA2}
Assess the quality of data releases (including the co-added image data products) performing quality assessment for astrometric and photometric calibration and derived products, looking for problems with the image processing pipelines and systematic problems with the instrument.
Validating the Data Release products.
All components from QA0

New main components:
\begin{enumerate}
\item DRP-specific dataset
\item Release data product editing tools (including provenance tracking)
\item Output interface to workflow system based on QA results and provenance
\item Provenance analysis tools
\item Output interface to Science Pipelines, including from QA database
\item Comparison tools for overlap areas due to satellite processing 
\item Metrics/products for science users to understand quality of science data products (depth mask/selection function, etc.)
\item Characterization report for Data Release
\end{enumerate}

\subsubsection{QA3}
Data quality based on science analysis performed by the LSST Science Collaborations and the community. Level 0-2 visualization and data exploration tools will be made available to the community.
Make all results from the above available. Make all of the above components available to some part of the community (could be just affiliated data centers or could be individual scientists) as a supported product.
Ingest external science analysis data as Level 3 data products; ingest useful external science analysis tools.

\subsubsection{Prototype Implementation of PipeQA}

The pipeQA prototype is a useful reference for exploring ideas and we mention
it here to capture this prototype work.

A prototype implementation of the SDQA was implemented in LSST Final Design Phase. The existing prototype was tested with image simulation inputs, as well as real data (SDSS Stripe 82).
\\

The prototype used a set of statically and dynamically generated pages (written in php) to display the results of data production runs. While proving invaluable for data analysis, the prototype design was found it to be difficult to extend with new analyst-developed tests.
\\

The prototype code is available in the \url{https://github.com/lsst/testing_displayQA} git repository.
